\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{WDADN22}

\bibitem[HM22]{higham2022mixed}
Nicholas~J. Higham and Theo Mary.
\newblock Mixed precision algorithms in numerical linear algebra.
\newblock {\em Acta Numerica}, 31:347--414, 2022.

\bibitem[{IEE}19]{ieee754-2019}
{IEEE}.
\newblock {IEEE} standard for floating-point arithmetic, 2019.

\bibitem[KMM{\etalchar{+}}19]{kalamkar2019bfloat16}
Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal
  Banerjee, Sasikanth Avancha, Dharma~Teja Vooturi, Nataraj Jammalamadaka,
  Jianyu Huang, Hector Yuen, et~al.
\newblock A study of {BFLOAT16} for deep learning training.
\newblock In {\em arXiv preprint arXiv:1905.12322}, 2019.

\bibitem[{NVI}20]{nvidia2020a100}
{NVIDIA Corporation}.
\newblock {\em {NVIDIA} {A100} Tensor Core {GPU} Architecture}, 2020.
\newblock Whitepaper v1.0.

\bibitem[{NVI}22]{nvidia2022h100}
{NVIDIA Corporation}.
\newblock {\em {NVIDIA} {H100} Tensor Core {GPU} Architecture}, 2022.
\newblock Whitepaper.

\bibitem[{NVI}24a]{cublas}
{NVIDIA Corporation}.
\newblock {cuBLAS} library, 2024.
\newblock \url{https://developer.nvidia.com/cublas}.

\bibitem[{NVI}24b]{nvidia-cuda-guide}
{NVIDIA Corporation}.
\newblock {\em {CUDA} {C}++ Programming Guide}, 2024.
\newblock \url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/}.

\bibitem[{NVI}24c]{cutensor}
{NVIDIA Corporation}.
\newblock {cuTENSOR}: A high-performance tensor primitives library, 2024.
\newblock \url{https://developer.nvidia.com/cutensor}.

\bibitem[{NVI}25]{cublasdx}
{NVIDIA Corporation}.
\newblock {cuBLASDx}: Device side {BLAS} extensions, 2025.
\newblock \url{https://docs.nvidia.com/cuda/cublasdx/}.

\bibitem[RH20]{nvidia2020gtc-ampere}
Ronny Ramirez and William Hsu.
\newblock Optimizing applications for {NVIDIA} {Ampere} {GPU} architecture.
\newblock GTC 2020, Session S21819, 2020.
\newblock
  \url{https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s21819-optimizing-applications-for-nvidia-ampere-gpu-architecture.pdf}.

\bibitem[SSK17]{springer2017tensor}
Paul Springer, Tong Su, and Tamara~G. Kolda.
\newblock Tensor contractions with extended {BLAS} kernels on {CPU} and {GPU}.
\newblock In {\em IEEE 24th International Conference on High Performance
  Computing (HiPC)}. IEEE, 2017.
\newblock
  \url{https://research.nvidia.com/sites/default/files/pubs/2017-10_Tensor-Contractions-with/tensors_hipc.pdf}.

\bibitem[WDADN22]{wu2022chase-gpu}
Xinzhe Wu, Davor Davidovi\'{c}, Sebastian Achilles, and Edoardo Di~Napoli.
\newblock {ChASE}: a distributed hybrid {CPU-GPU} eigensolver for large-scale
  hermitian eigenvalue problems.
\newblock In {\em Proceedings of the Platform for Advanced Scientific Computing
  Conference (PASC~'22)}, pages 1--12. ACM, 2022.

\bibitem[WDN23]{wu2023chase-nccl}
Xinzhe Wu and Edoardo Di~Napoli.
\newblock Advancing the distributed multi-{GPU} {ChASE} library through
  algorithm optimization and {NCCL} library.
\newblock In {\em Workshops of The International Conference on High Performance
  Computing, Network, Storage, and Analysis (SC-W~'23)}, pages 1688--1696. ACM,
  2023.

\bibitem[WSDN19]{winkelmann2019chase}
Jan Winkelmann, Paul Springer, and Edoardo Di~Napoli.
\newblock {ChASE}: {C}hebyshev accelerated subspace iteration eigensolver for
  sequences of {H}ermitian eigenvalue problems.
\newblock {\em ACM Transactions on Mathematical Software}, 45(2):1--34, 2019.

\bibitem[Wu19]{wu2019phd}
Xinzhe Wu.
\newblock {\em Contribution to the Emergence of New Intelligent Parallel and
  Distributed Methods Using a Multi-level Programming Paradigm for Extreme
  Computing}.
\newblock PhD thesis, University of Lille, 2019.

\end{thebibliography}
