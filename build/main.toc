\babel@toc {english}{}\relax 
\contentsline {chapter}{Abstract}{iii}{chapter*.1}%
\contentsline {chapter}{Acknowledgements}{v}{chapter*.2}%
\contentsline {chapter}{\nonumberline List of Figures}{ix}{chapter*.4}%
\contentsline {chapter}{\nonumberline List of Tables}{xi}{chapter*.5}%
\contentsline {chapter}{\nonumberline Listings}{xiii}{chapter*.6}%
\contentsline {chapter}{List of Symbols}{xv}{chapter*.7}%
\contentsline {chapter}{List of Abbreviations}{xvii}{chapter*.8}%
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Problem Statement}{1}{section.1.2}%
\contentsline {section}{\numberline {1.3}Contributions}{1}{section.1.3}%
\contentsline {section}{\numberline {1.4}Outline}{1}{section.1.4}%
\contentsline {chapter}{\numberline {2}Background}{3}{chapter.2}%
\contentsline {section}{\numberline {2.1}Tensor Networks}{3}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Tensor Notation and Diagrams}{3}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Tensor Contraction}{3}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}Tensor Network Structures}{3}{subsection.2.1.3}%
\contentsline {section}{\numberline {2.2}GPU Architecture}{3}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Streaming Multiprocessor and Warp Execution}{3}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Floating-Point Formats and Precision Trade-offs}{3}{subsection.2.2.2}%
\contentsline {subsubsection}{\nonumberline IEEE 754 Binary Representation}{3}{subsubsection*.10}%
\contentsline {subsubsection}{\nonumberline A100 Throughput by Precision}{5}{subsubsection*.15}%
\contentsline {subsubsection}{\nonumberline The Case for Single Precision in Tensor Network Computations}{5}{subsubsection*.18}%
\contentsline {subsection}{\numberline {2.2.3}Memory Hierarchy}{6}{subsection.2.2.3}%
\contentsline {paragraph}{\nonumberline GPU Memory Hierarchy}{7}{figure.caption.22}%
\contentsline {subsection}{\numberline {2.2.4}NVIDIA A100 Ampere Architecture}{7}{subsection.2.2.4}%
\contentsline {subsubsection}{\nonumberline Hardware Overview}{7}{subsubsection*.24}%
\contentsline {subsubsection}{\nonumberline Theoretical Peak Performance}{8}{subsubsection*.27}%
\contentsline {subsubsection}{\nonumberline Derived Performance Limits}{8}{subsubsection*.30}%
\contentsline {subsubsection}{\nonumberline Memory Latency}{8}{subsubsection*.33}%
\contentsline {section}{\numberline {2.3}CUDA Programming Model}{8}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Thread Hierarchy and Kernel Launch}{8}{subsection.2.3.1}%
\contentsline {subsubsection}{\nonumberline Kernel Syntax and Launch Configuration}{10}{subsubsection*.37}%
\contentsline {subsubsection}{\nonumberline Two-Dimensional Grid Addressing}{10}{subsubsection*.39}%
\contentsline {subsubsection}{\nonumberline Block Size Selection and Hardware Constraints}{11}{subsubsection*.41}%
\contentsline {subsubsection}{\nonumberline Linearisation of Multidimensional Indices}{11}{subsubsection*.43}%
\contentsline {subsection}{\numberline {2.3.2}Shared Memory and Synchronisation}{11}{subsection.2.3.2}%
\contentsline {subsubsection}{\nonumberline Static and Dynamic Allocation}{12}{subsubsection*.45}%
\contentsline {subsubsection}{\nonumberline Tiled Matrix Multiplication}{12}{subsubsection*.47}%
\contentsline {subsection}{\numberline {2.3.3}Memory Coalescing and Bank Conflicts}{13}{subsection.2.3.3}%
\contentsline {subsubsection}{\nonumberline Global Memory Coalescing}{13}{subsubsection*.49}%
\contentsline {subsubsection}{\nonumberline Shared Memory Bank Conflicts}{14}{subsubsection*.51}%
\contentsline {subsection}{\numberline {2.3.4}Performance Profiling with Nsight Compute}{14}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}Related Work}{15}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}cuBLAS and cuTENSOR}{15}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}ChASE Eigensolver}{15}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Existing GPU Tensor Network Implementations}{15}{subsection.2.4.3}%
\contentsline {chapter}{\numberline {3}Design and Methodology}{17}{chapter.3}%
\contentsline {section}{\numberline {3.1}Target Kernels}{17}{section.3.1}%
\contentsline {section}{\numberline {3.2}Algorithmic Approach}{17}{section.3.2}%
\contentsline {section}{\numberline {3.3}Data Layout and Memory Strategy}{17}{section.3.3}%
\contentsline {section}{\numberline {3.4}Baseline Selection}{17}{section.3.4}%
\contentsline {chapter}{\numberline {4}Implementation}{19}{chapter.4}%
\contentsline {section}{\numberline {4.1}Kernel Design}{19}{section.4.1}%
\contentsline {section}{\numberline {4.2}Shared Memory Tiling}{19}{section.4.2}%
\contentsline {section}{\numberline {4.3}Occupancy and Launch Configuration}{19}{section.4.3}%
\contentsline {section}{\numberline {4.4}Integration and Build System}{19}{section.4.4}%
\contentsline {chapter}{\numberline {5}Results}{21}{chapter.5}%
\contentsline {section}{\numberline {5.1}Experimental Setup}{21}{section.5.1}%
\contentsline {section}{\numberline {5.2}Single-GPU Performance}{21}{section.5.2}%
\contentsline {section}{\numberline {5.3}Profiling Analysis}{21}{section.5.3}%
\contentsline {section}{\numberline {5.4}Comparison with cuBLAS and cuTENSOR}{21}{section.5.4}%
\contentsline {section}{\numberline {5.5}Scaling Behaviour}{21}{section.5.5}%
\contentsline {section}{\numberline {5.6}Discussion}{21}{section.5.6}%
\contentsline {chapter}{\numberline {6}Conclusion}{23}{chapter.6}%
\contentsline {section}{\numberline {6.1}Summary}{23}{section.6.1}%
\contentsline {section}{\numberline {6.2}Limitations}{23}{section.6.2}%
\contentsline {section}{\numberline {6.3}Future Work}{23}{section.6.3}%
\contentsline {chapter}{\nonumberline Bibliography}{25}{chapter*.52}%
\contentsline {chapter}{\numberline {A}Supplementary Benchmarks}{27}{appendix.A}%
\contentsline {chapter}{Declaration of Authorship}{29}{chapter*.53}%
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
