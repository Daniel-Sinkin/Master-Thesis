\section{Case Study: Contraction Order in a Two-Site TN Update}
\label{sec:tn-contraction-order-case}

This section introduces a concrete tensor-network-style problem where a
reasonable first implementation can be much slower than a mathematically
equivalent alternative.

\subsection{Problem Definition}

Consider a batched two-site update with fused physical dimension $d_p=d^2$:
\[
  Y_{b,p,r}
  =
  \sum_{q=0}^{d_p-1}\sum_{c=0}^{\chi-1}
  G_{p,q}\,X_{b,q,c}\,M_{c,r},
\]
where:
\begin{itemize}
  \item $b\in[0,B)$ is a batch index,
  \item $p,q\in[0,d_p)$ are fused physical indices,
  \item $c,r\in[0,\chi)$ are bond-space indices.
\end{itemize}

The tensor structure is representative of two-site gate application and local
environment projection steps in MPS-style workflows.

\subsection{Plausible Baseline (Bad)}

A straightforward GPU implementation computes one output element
$Y_{b,p,r}$ per thread and evaluates both sums directly. This gives a direct
cost
\[
  \BigO\!\left(B\,d_p^2\,\chi^2\right),
\]
with weak reuse of $X$ and $M$ across threads and heavy global-memory traffic.
This baseline is implemented in
\path{code/profiling/tn_two_site_bad_direct.cu}.

\subsection{Improved Algorithm (Better)}

The key change is contraction order:
\[
  T_{b,p,c} = \sum_{q=0}^{d_p-1} G_{p,q}X_{b,q,c},
  \qquad
  Y_{b,p,r} = \sum_{c=0}^{\chi-1} T_{b,p,c}M_{c,r}.
\]
This reduces asymptotic work to
\[
  \BigO\!\left(B\,(d_p^2\chi + d_p\chi^2)\right),
\]
and maps directly to two strided-batched GEMMs. The implementation is
\path{code/profiling/tn_two_site_good_batched_gemm.cu}.

\begin{figure}[H]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tikzpicture}[
    >=Latex,
    every node/.style={font=\small, align=center},
    tensor/.style={draw, rounded corners=2pt, fill=diagBlue, minimum width=1.8cm, minimum height=0.9cm},
    opbad/.style={draw, rounded corners=2pt, fill=diagRed, minimum width=2.5cm, minimum height=1.0cm},
    opgood/.style={draw, rounded corners=2pt, fill=diagGreen, minimum width=2.5cm, minimum height=1.0cm},
    cost/.style={draw, rounded corners=2pt, fill=diagYellow, inner sep=3pt},
    arrow/.style={->, very thick}
  ]
    \node[font=\bfseries] at (3.2,2.1) {Direct baseline (single fused loop nest)};
    \node[tensor] (xb) at (0.8,1.2) {$X$};
    \node[tensor] (gb) at (2.4,1.2) {$G$};
    \node[tensor] (mb) at (4.0,1.2) {$M$};
    \node[opbad] (db) at (2.4,0.0) {Direct\\contraction};
    \node[tensor] (yb) at (2.4,-1.2) {$Y$};
    \draw[arrow] (xb) -- (db);
    \draw[arrow] (gb) -- (db);
    \draw[arrow] (mb) -- (db);
    \draw[arrow] (db) -- (yb);
    \node[cost] at (2.4,-2.05) {$\BigO(B\,d_p^2\chi^2)$};

    \node[font=\bfseries] at (11.2,2.1) {Ordered contraction (two GEMM stages)};
    \node[tensor] (xg) at (8.8,1.2) {$X$};
    \node[tensor] (gg) at (10.4,1.2) {$G$};
    \node[opgood] (g1) at (9.6,0.0) {Stage 1\\$T=G\cdot X$};
    \node[tensor] (tg) at (11.2,0.0) {$T$};
    \node[tensor] (mg) at (12.8,0.0) {$M$};
    \node[opgood] (g2) at (12.0,-1.2) {Stage 2\\$Y=T\cdot M$};
    \node[tensor] (yg) at (12.0,-2.3) {$Y$};
    \draw[arrow] (xg) -- (g1);
    \draw[arrow] (gg) -- (g1);
    \draw[arrow] (g1) -- (tg);
    \draw[arrow] (tg) -- (g2);
    \draw[arrow] (mg) -- (g2);
    \draw[arrow] (g2) -- (yg);
    \node[cost] at (12.0,-3.15) {$\BigO(B(d_p^2\chi + d_p\chi^2))$};
  \end{tikzpicture}%
  }
  \caption{Algorithmic structure of the two-site case study: direct contraction
  (left) versus contraction-order decomposition into two GEMM-like stages
  (right).}
  \label{fig:tn-case-contraction-order-structure}
\end{figure}

Theoretical operation-ratio improvement (direct vs ordered) is
\[
  R
  =
  \frac{d_p^2\chi^2}{d_p^2\chi + d_p\chi^2}
  =
  \frac{d_p\chi}{d_p+\chi}.
\]
For the benchmarked shape $d_p=16$, $\chi=256$, this gives
$R\approx 15.1$, before any hardware-level optimisations.

\subsection{Design Decisions Behind the Better Variant}

The implementation sequence follows the staged optimisation style from
Simon B\"ohm's SGEMM worklog~\cite{boehm2022cuda-mmm}:
start with algorithmic structure, then improve memory behavior, then improve
execution mapping.

\begin{table}[H]
  \centering
  \small
  \begin{tabularx}{\textwidth}{l X}
    \toprule
    Design decision & Why it helps \\
    \midrule
    Change contraction order first & Removes redundant work
    ($\BigO(Bd_p^2\chi^2)\rightarrow\BigO(B(d_p^2\chi+d_p\chi^2))$). \\
    Map to strided-batched GEMM & Uses highly tuned kernels instead of manual
    scalar loops. \\
    Reuse shared operands with zero stride & $G$ and $M$ are reused across the
    whole batch without host-side relaunch loops. \\
    Keep FP32 pedantic math mode & Preserves strict FP32 behavior while
    comparing algorithmic variants. \\
    Profile with fixed KPI pack & Separates true algorithm gains from incidental
    launch/measurement noise. \\
    \bottomrule
  \end{tabularx}
  \caption{Main design decisions for the two-site contraction case study.}
  \label{tab:tn-case-design-decisions}
\end{table}

\subsection{Optimisation Ladder (Structured After SGEMM Worklogs)}

Beyond the single ``bad vs good'' comparison, this case follows a staged
optimisation structure similar to the SGEMM worklog style in
\cite{boehm2022cuda-mmm}: fix algorithmic work first, then remove memory
pathologies, then improve scheduling and occupancy, and finally tune.

\begin{enumerate}
  \item \textbf{Stage 0 (correct baseline).} Start from a slow but clearly
  correct direct contraction to define a reproducible reference.
  \item \textbf{Stage 1 (contraction order).} Remove redundant arithmetic in
  naive loop nests first; this is often the largest single gain for TN updates.
  \item \textbf{Stage 2 (global-memory access).} Fix non-coalesced and heavily
  strided operand traversal so traffic scales with useful work.
  \item \textbf{Stage 3 (on-chip reuse).} Avoid re-reading hot operands from
  HBM by increasing L2/shared-memory reuse where possible.
  \item \textbf{Stage 4 (scheduler feed / ILP).} Increase independent work per
  warp to improve eligible-warps and issue efficiency.
  \item \textbf{Stage 5 (resource balance).} Control register growth so
  occupancy does not collapse under aggressive unrolling/blocking.
  \item \textbf{Stage 6 (launch cost).} Fuse tiny pre/post kernels around the
  contraction to amortise host launch overhead.
  \item \textbf{Stage 7 (parameter search).} Tune tile/shape mappings per
  regime, since good choices are hardware- and size-dependent.
\end{enumerate}

\subsection{Profiling Reproducibility}

The complete compile-and-profile job is provided as
\path{code/profiling/ncu_tn_contraction_profile.slurm}.
It collects the same KPI pack used in the rest of this thesis and writes
separate CSV files for both variants.
