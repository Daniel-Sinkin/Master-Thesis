\section{Case Study: Contraction Order in a Two-Site TN Update}
\label{sec:tn-contraction-order-case}

A concrete tensor-network-style case shows how a reasonable first
implementation can be much slower than a mathematically equivalent
alternative.

\subsection{Problem Definition}

Consider a batched two-site update with fused physical dimension $d_p=d^2$:
\[
  Y_{b,p,r}
  =
  \sum_{q=0}^{d_p-1}\sum_{c=0}^{\chi-1}
  G_{p,q}\,X_{b,q,c}\,M_{c,r},
\]
where:
\begin{itemize}
  \item $b\in[0,B)$ is a batch index,
  \item $p,q\in[0,d_p)$ are fused physical indices,
  \item $c,r\in[0,\chi)$ are bond-space indices.
\end{itemize}

The tensor structure is representative of two-site gate application and local
environment projection steps in MPS-style workflows.

\subsection{How to Read This as a Practical HPC Task}

The expression can be read as a matrix-chain computation repeated over a batch:
\begin{itemize}
  \item for each fixed batch index $b$, interpret $X_{b,\cdot,\cdot}$ as a
  matrix $X_b\in\mathbb{R}^{d_p\times \chi}$,
  \item interpret $G$ as $d_p\times d_p$ and $M$ as $\chi\times\chi$,
  \item compute
  \[
    Y_b = (G\,X_b)\,M,\qquad b=0,\dots,B-1.
  \]
\end{itemize}

From an implementation point of view, the core is a repeated dense
linear-algebra pattern where contraction order decides both arithmetic work and
memory traffic.

\begin{figure}[H]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tikzpicture}[
    >=Latex,
    every node/.style={font=\small, align=center},
    mat/.style={draw, rounded corners=2pt, fill=diagBlue, minimum width=2.8cm, minimum height=1.0cm},
    stagebox/.style={draw, rounded corners=2pt, fill=diagGreen, minimum width=2.4cm, minimum height=1.0cm},
    arrow/.style={->, very thick}
  ]
    \node[mat] (g) at (1.6,0) {$G$\\$(d_p\times d_p)$};
    \node[mat] (x) at (5.4,0) {$X_b$\\$(d_p\times \chi)$};
    \node[stagebox] (s1) at (8.8,0) {Stage 1};
    \node[mat] (t) at (12.2,0) {$T_b$\\$(d_p\times \chi)$};
    \node[mat] (m) at (15.6,0) {$M$\\$(\chi\times \chi)$};
    \node[stagebox] (s2) at (19.0,0) {Stage 2};
    \node[mat] (y) at (22.4,0) {$Y_b$\\$(d_p\times \chi)$};

    \draw[arrow] (g) -- node[above]{GEMM $(m,n,k)=(d_p,\chi,d_p)$} (s1);
    \draw[arrow] (x) -- (s1);
    \draw[arrow] (s1) -- (t);
    \draw[arrow] (t) -- node[above]{GEMM $(m,n,k)=(d_p,\chi,\chi)$} (s2);
    \draw[arrow] (m) -- (s2);
    \draw[arrow] (s2) -- (y);
  \end{tikzpicture}%
  }
  \caption{Per-batch matrix view of the two-site update. The tensor expression
  is equivalent to a two-stage GEMM chain with a shared $G$ and $M$ across
  batches.}
  \label{fig:tn-case-matrix-view}
\end{figure}

\subsection{Plausible Baseline (Bad)}

A straightforward GPU implementation computes one output element
$Y_{b,p,r}$ per thread and evaluates both sums directly. This gives a direct
cost
\[
  \BigO\!\left(B\,d_p^2\,\chi^2\right),
\]
with weak reuse of $X$ and $M$ across threads and heavy global-memory traffic.
This baseline is implemented in
\path{code/profiling/tn_two_site_bad_direct.cu}.

\subsection{Improved Algorithm (Better)}

The key change is contraction order:
\[
  T_{b,p,c} = \sum_{q=0}^{d_p-1} G_{p,q}X_{b,q,c},
  \qquad
  Y_{b,p,r} = \sum_{c=0}^{\chi-1} T_{b,p,c}M_{c,r}.
\]
This reduces asymptotic work to
\[
  \BigO\!\left(B\,(d_p^2\chi + d_p\chi^2)\right),
\]
and maps directly to two strided-batched GEMMs. The implementation is
\path{code/profiling/tn_two_site_good_batched_gemm.cu}.

\begin{figure}[H]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tikzpicture}[
    >=Latex,
    every node/.style={font=\small, align=center},
    tensor/.style={draw, rounded corners=2pt, fill=diagBlue, minimum width=1.8cm, minimum height=0.9cm},
    opbad/.style={draw, rounded corners=2pt, fill=diagRed, minimum width=2.5cm, minimum height=1.0cm},
    opgood/.style={draw, rounded corners=2pt, fill=diagGreen, minimum width=2.5cm, minimum height=1.0cm},
    cost/.style={draw, rounded corners=2pt, fill=diagYellow, inner sep=3pt},
    arrow/.style={->, very thick}
  ]
    \node[font=\bfseries] at (3.2,2.1) {Direct baseline (single fused loop nest)};
    \node[tensor] (xb) at (0.8,1.2) {$X$};
    \node[tensor] (gb) at (2.4,1.2) {$G$};
    \node[tensor] (mb) at (4.0,1.2) {$M$};
    \node[opbad] (db) at (2.4,0.0) {Direct\\contraction};
    \node[tensor] (yb) at (2.4,-1.2) {$Y$};
    \draw[arrow] (xb) -- (db);
    \draw[arrow] (gb) -- (db);
    \draw[arrow] (mb) -- (db);
    \draw[arrow] (db) -- (yb);
    \node[cost] at (2.4,-2.05) {$\BigO(B\,d_p^2\chi^2)$};

    \node[font=\bfseries] at (11.2,2.1) {Ordered contraction (two GEMM stages)};
    \node[tensor] (xg) at (8.8,1.2) {$X$};
    \node[tensor] (gg) at (10.4,1.2) {$G$};
    \node[opgood] (g1) at (9.6,0.0) {Stage 1\\$T=G\cdot X$};
    \node[tensor] (tg) at (11.2,0.0) {$T$};
    \node[tensor] (mg) at (12.8,0.0) {$M$};
    \node[opgood] (g2) at (12.0,-1.2) {Stage 2\\$Y=T\cdot M$};
    \node[tensor] (yg) at (12.0,-2.3) {$Y$};
    \draw[arrow] (xg) -- (g1);
    \draw[arrow] (gg) -- (g1);
    \draw[arrow] (g1) -- (tg);
    \draw[arrow] (tg) -- (g2);
    \draw[arrow] (mg) -- (g2);
    \draw[arrow] (g2) -- (yg);
    \node[cost] at (12.0,-3.15) {$\BigO(B(d_p^2\chi + d_p\chi^2))$};
  \end{tikzpicture}%
  }
  \caption{Algorithmic structure of the two-site case study: direct contraction
  (left) versus contraction-order decomposition into two GEMM-like stages
  (right).}
  \label{fig:tn-case-contraction-order-structure}
\end{figure}

Theoretical operation-ratio improvement (direct vs ordered) is
\[
  R
  =
  \frac{d_p^2\chi^2}{d_p^2\chi + d_p\chi^2}
  =
  \frac{d_p\chi}{d_p+\chi}.
\]
For the benchmarked shape $d_p=16$, $\chi=256$, this gives
$R\approx 15.1$, before any hardware-level optimisations.

\subsection{Worked Numerical Example (Shape Used in This Thesis)}

Using the benchmarked shape $B=1$, $d_p=16$, $\chi=256$:
\begin{itemize}
  \item output size is $d_p\chi=4096$ elements,
  \item direct variant evaluates $d_p\chi=4096$ inner terms per output element,
  \item total direct term evaluations are
  \[
    N_{\text{direct}} = d_p^2\chi^2 = 16{,}777{,}216.
  \]
\end{itemize}

For ordered contraction:
\[
  N_{\text{stage1}} = d_p^2\chi = 65{,}536,\qquad
  N_{\text{stage2}} = d_p\chi^2 = 1{,}048{,}576,
\]
so
\[
  N_{\text{ordered}} = N_{\text{stage1}}+N_{\text{stage2}} = 1{,}114{,}112.
\]
This gives
\[
  \frac{N_{\text{direct}}}{N_{\text{ordered}}}\approx 15.06,
\]
which matches the asymptotic ratio above.

\begin{table}[H]
  \centering
  \small
  \begin{tabularx}{\textwidth}{l r r}
    \toprule
    Variant & Term evaluations & Relative to ordered \\
    \midrule
    Direct contraction & $16{,}777{,}216$ & $15.06\times$ \\
    Ordered contraction (two-stage) & $1{,}114{,}112$ & $1.00\times$ \\
    \bottomrule
  \end{tabularx}
  \caption{Work comparison for the thesis reference shape ($B=1$, $d_p=16$,
  $\chi=256$).}
  \label{tab:tn-case-worked-example}
\end{table}

\subsection{Practice Problems (Implementation-Oriented)}

\begin{enumerate}
  \item \textbf{Work counting.} For $B=4$, $d_p=32$, $\chi=64$, compute
  $N_{\text{direct}}$ and $N_{\text{ordered}}$.
  \item \textbf{GEMM mapping.} Write the two GEMMs in $(m,n,k)$ form for stage 1
  and stage 2, and identify which operands are shared across the batch.
  \item \textbf{Shape intuition.} Using
  $R=\frac{d_p\chi}{d_p+\chi}$, determine whether increasing $d_p$ or
  increasing $\chi$ helps more when $\chi\gg d_p$.
  \item \textbf{Memory layout choice.} Pick a layout where the hottest
  contracted index is unit stride in both stages, and state one layout
  transform needed between stage 1 and stage 2.
\end{enumerate}

\paragraph{Short answer key.}
\begin{enumerate}
  \item $N_{\text{direct}}=16{,}777{,}216$,
  $N_{\text{ordered}}=786{,}432$ ($\approx 21.3\times$ reduction).
  \item Stage 1: $(m,n,k)=(d_p,\chi,d_p)$, Stage 2:
  $(m,n,k)=(d_p,\chi,\chi)$; $G$ and $M$ are batch-shared.
  \item For $\chi\gg d_p$, ratio $R\approx d_p$, so increasing $d_p$ gives the
  stronger marginal gain.
  \item Any valid answer should keep the reduction index contiguous in the
  innermost access pattern of each stage.
\end{enumerate}

\subsection{Diagram Blueprint for Manual Drawing}

To explain this section in one slide or one page, a compact three-panel diagram
works well:
\begin{enumerate}
  \item \textbf{Panel A: expression view.} Write
  $Y_{b,p,r}=\sum_{q,c}G_{p,q}X_{b,q,c}M_{c,r}$ and highlight contracted
  indices ($q,c$) in one color.
  \item \textbf{Panel B: direct implementation.} Draw one big ``direct
  contraction'' block with arrows from $G$, $X$, and $M$ to $Y$; annotate
  $\BigO(Bd_p^2\chi^2)$.
  \item \textbf{Panel C: ordered implementation.} Draw two blocks:
  $T=G\cdot X$ then $Y=T\cdot M$; annotate
  $\BigO(B(d_p^2\chi+d_p\chi^2))$ and the ratio
  $\frac{d_p\chi}{d_p+\chi}$.
\end{enumerate}

For readability in Excalidraw, keep only five tensor nodes
($G,X,M,T,Y$), color contracted indices consistently, and place the work
numbers directly below each panel.

\subsection{Design Decisions Behind the Better Variant}

The implementation sequence follows the staged optimisation style from
Simon B\"ohm's SGEMM worklog~\cite{boehm2022cuda-mmm}:
start with algorithmic structure, then improve memory behavior, then improve
execution mapping.

\begin{table}[H]
  \centering
  \small
  \begin{tabularx}{\textwidth}{l X}
    \toprule
    Design decision & Why it helps \\
    \midrule
    Change contraction order first & Removes redundant work
    ($\BigO(Bd_p^2\chi^2)\rightarrow\BigO(B(d_p^2\chi+d_p\chi^2))$). \\
    Map to strided-batched GEMM & Uses highly tuned kernels instead of manual
    scalar loops. \\
    Reuse shared operands with zero stride & $G$ and $M$ are reused across the
    whole batch without host-side relaunch loops. \\
    Keep FP32 pedantic math mode & Preserves strict FP32 behavior while
    comparing algorithmic variants. \\
    Profile with fixed KPI pack & Separates true algorithm gains from incidental
    launch/measurement noise. \\
    \bottomrule
  \end{tabularx}
  \caption{Main design decisions for the two-site contraction case study.}
  \label{tab:tn-case-design-decisions}
\end{table}

\subsection{Optimisation Ladder (Structured After SGEMM Worklogs)}

Beyond the single ``bad vs good'' comparison, this case follows a staged
optimisation structure similar to the SGEMM worklog style in
\cite{boehm2022cuda-mmm}: fix algorithmic work first, then remove memory
pathologies, then improve scheduling and occupancy, and finally tune.

\begin{enumerate}
  \item \textbf{Stage 0 (correct baseline).} Start from a slow but clearly
  correct direct contraction to define a reproducible reference.
  \item \textbf{Stage 1 (contraction order).} Remove redundant arithmetic in
  naive loop nests first; this is often the largest single gain for TN updates.
  \item \textbf{Stage 2 (global-memory access).} Fix non-coalesced and heavily
  strided operand traversal so traffic scales with useful work.
  \item \textbf{Stage 3 (on-chip reuse).} Avoid re-reading hot operands from
  HBM by increasing L2/shared-memory reuse where possible.
  \item \textbf{Stage 4 (scheduler feed / ILP).} Increase independent work per
  warp to improve eligible-warps and issue efficiency.
  \item \textbf{Stage 5 (resource balance).} Control register growth so
  occupancy does not collapse under aggressive unrolling/blocking.
  \item \textbf{Stage 6 (launch cost).} Fuse tiny pre/post kernels around the
  contraction to amortise host launch overhead.
  \item \textbf{Stage 7 (parameter search).} Tune tile/shape mappings per
  regime, since good choices are hardware- and size-dependent.
\end{enumerate}

\subsection{Profiling Reproducibility}

The complete compile-and-profile job is provided as
\path{code/profiling/ncu_tn_contraction_profile.slurm}.
It collects the same KPI pack used in the rest of this thesis and writes
separate CSV files for both variants.
