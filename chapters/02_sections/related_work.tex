% chapters/02_sections/related_work.tex
\section{Related Work}\label{sec:related-work}

\subsection{cuBLAS, cuBLASDx, and cuTENSOR}\label{sec:cublas-cublasdx-cutensor}

NVIDIA's vendor libraries define the practical baseline for dense kernels on
Ampere GPUs. cuBLAS provides highly optimised BLAS routines and remains the
reference point for GEMM-like workloads~\cite{cublas}. cuTENSOR generalises
this to tensor contractions and permutation-heavy primitives~\cite{cutensor}.

For the small-kernel regime relevant to tensor-network workflows, cuBLASDx is
especially important: it enables device-side GEMM composition and supports
launch-amortisation strategies that are difficult to implement with host-only
library calls~\cite{cublasdx}. The launch-overhead benchmark in this
thesis (\cref{sec:launch-overhead-benchmark}) uses this capability directly.

In this thesis, custom kernels are evaluated against these libraries as
performance baselines rather than replacements. The objective is to understand
where workload-specific structure can beat general-purpose interfaces.

\subsection{ChASE Eigensolver}\label{sec:chase}

The ChASE line of work provides relevant experience in GPU-oriented numerical
software engineering, especially for hybrid CPU--GPU execution and scalable
performance tuning~\cite{winkelmann2019chase,wu2022chase-gpu,wu2023chase-nccl}.
Although ChASE targets eigenvalue problems rather than tensor networks, it
shares core HPC concerns with this thesis:

\begin{itemize}
  \item balancing algorithmic structure with hardware-aware implementation,
  \item minimising communication and orchestration overhead,
  \item validating performance claims with rigorous benchmarking.
\end{itemize}

These principles influence the methodological choices in later chapters,
particularly the emphasis on bottleneck-driven optimisation.

\subsection{Existing GPU Tensor Network Implementations}\label{sec:existing-tn-gpu}

Prior work on tensor contractions shows that performance is sensitive to index
ordering and data layout, plus the mapping of contractions to BLAS kernels
(\cite{springer2017tensor} provides an early and still useful reference point).
Across implementations, two recurring observations are relevant here:

\begin{enumerate}
  \item \textbf{Kernel granularity matters.} Many contractions are too small to
    fully utilise GPU throughput when issued individually.
  \item \textbf{Data movement dominates.} Layout transforms and non-coalesced
    accesses can erase gains from nominally fast compute kernels.
\end{enumerate}

This thesis builds on those observations with a stricter focus on Ampere-era
execution details: launch overhead, occupancy limits, memory-hierarchy effects,
plus profile-guided fusion on a realistic node topology.

\subsection{Selected Work in Collaborating Research Directions}\label{sec:people-sota}

The following review focuses on work that is directly relevant to this thesis
context: Wu, Di~Napoli, Rizzi, Waintal, Legeza, and their close collaborators.

\subsubsection{Wu and Di Napoli: GPU-Distributed Dense Kernels and Communication}

Wu et al.\ report a distributed hybrid CPU--GPU eigensolver where the dominant
kernel is a distributed Hermitian matrix--matrix multiplication step
(\texttt{HEMM}), and show that replacing vendor \texttt{PZGEMM} with a custom
GPU-distributed implementation significantly improves strong scaling on
JUWELS-Booster~\cite{wu2022chase-gpu}. The SC-W extension further improves
distributed behaviour with NCCL-based communication
optimisations~\cite{wu2023chase-nccl}.

A practical architectural lesson is clear: once local kernels are efficient,
communication orchestration and overlap become major bottlenecks.

\subsubsection{Legeza Group: Massively Parallel DMRG/MPS on CPU--GPU Systems}

Menczer et al.\ present a sequence of works on massively parallel DMRG-style
workloads that are highly relevant for kernel engineering: lock-free scheduling,
task buffering, explicit overlap of communication and computation, and
mixed-precision/tensor-core acceleration paths
(\cite{menczer2023massive,menczer2023nonabelian,menczer2024efficient-dmrg-gpu}).

The 2023 and 2024 studies also report good scaling across heterogeneous
CPU--GPU systems and explicitly motivate mixed-precision/tensor-core execution
for contraction-heavy workloads~\cite{menczer2023massive,menczer2024efficient-dmrg-gpu}.
For this thesis, the direct transfer is straightforward: reduce launch count for
small contractions, bucket by shape, then tune stream-level scheduling.

\subsubsection{Rizzi and Waintal: Operator-Sum Formulations and Practical TN Performance}

Krinitsin, Rizzi, and Waintal present a TTN time-evolution implementation with
two-level parallelisation (shared and distributed memory) and report order of
magnitude speedups from GPU acceleration in the targeted workload
regime~\cite{krinitsin2025ttn-ed}. A practical point for this thesis is their
emphasis on local operator-sum formulations, which can avoid expensive explicit
intermediate constructions in contraction pipelines.

Waintal and collaborators provide broader algorithmic limits and scalable
simulation perspectives for circuit-style tensor-network workloads
(\cite{zhou2020limits,ayral2023dmrg-circuit-fidelity}).
Even where papers are not CUDA-kernel papers, the computational patterns they
identify (many medium/small contractions, truncation-heavy iterative sweeps)
match the exact regime where launch overhead and layout efficiency matter most,
with batching as the main countermeasure on A100-class hardware.

\subsubsection{A100 CUDA Optimisation Practice (Documentation SOTA)}

For low-level CUDA engineering on Ampere, the most useful guidance comes
from NVIDIA's architecture and profiling documentation rather than from
application papers. The recurring recommendations are:
\begin{enumerate}
  \item treat memory traffic and reuse as first-order constraints
    (coalescing, shared-memory tiling, and cache-aware layouts),
  \item use occupancy as a constraint, not a sole objective; register and
    shared-memory pressure must be balanced against instruction-level
    efficiency,
  \item reduce launch overhead for short kernels through batching, fusion, and
    stream-graph style execution where appropriate,
  \item validate each optimisation with metric-level profiling instead of
    relying on throughput numbers alone.
\end{enumerate}
These points are consistent across the Ampere tuning guide, the CUDA
best-practices guide, and the Nsight Compute profiling guide
(\cite{nvidia-ampere-tuning-guide,nvidia-cuda-best-practices,nvidia-nsight-compute-profiling-guide}).

\paragraph{Integration plan used in this thesis.}
Given the current project scope (single A100 node, contraction-heavy short
kernel sequences), the most relevant near-term integration steps are:
\begin{enumerate}
  \item build a shape-bucketed benchmark suite for representative contraction
    sizes,
  \item maintain a per-kernel profiler checklist (occupancy, memory throughput,
    stall breakdown, launch cost),
  \item prioritise launch amortisation before aggressive micro-optimisation for
    kernels that run in the sub-millisecond regime,
  \item keep FP32 as baseline and treat TF32 as an opt-in acceleration path
    with explicit numerical checks.
\end{enumerate}

\subsubsection{Recurring Implementation Patterns}

Across the above works, four recurring implementation patterns emerge:

\begin{enumerate}
  \item \textbf{Task bucketing by contraction shape.} Grouping same-shape small
    contractions enables batched GEMM and reduces launch count.
  \item \textbf{Operator-sum execution paths.} Avoiding explicit large operator
    objects can reduce memory traffic and intermediate-kernel overhead.
  \item \textbf{Communication--computation overlap.} Multi-GPU execution
    benefits from explicit stream partitioning and asynchronous collectives.
  \item \textbf{Mixed-precision acceleration with guarded validation.} FP32/TF32
    tensor-core execution is used for throughput, with strict numerical checks.
\end{enumerate}

\subsubsection{Paper Notes for Implementation Planning (Working Draft)}
\label{sec:paper-notes-working-draft}

The table below is kept as implementation-oriented notes. It maps paper-level
results and methodology to concrete optimisation actions for this thesis.

\begin{table}[H]
\centering
\caption{Working notes: transfer from selected literature to CUDA optimisation actions in this thesis.}
\label{tab:paper-transfer-notes}
\begin{tabularx}{\textwidth}{X X X}
\toprule
Paper (focus) & Reported methodology/result signal & Direct action for this thesis \\
\midrule
\cite{menczer2023massive} (massively parallel TN state algorithms on CPU--GPU systems) &
explicit multi-level parallelisation and mixed CPU/GPU execution for contraction-heavy workloads &
keep kernels bucketed by shape and preserve stream-level concurrency; avoid one-kernel-per-contraction execution for small blocks \\
\cite{menczer2023nonabelian} (symmetry-aware massively parallel TN implementation) &
algorithmic structure strongly influences effective performance, not just raw kernel throughput &
separate profiling by contraction class and layout class; avoid aggregating all kernels into one average metric \\
\cite{menczer2024efficient-dmrg-gpu} (DMRG implementation/tuning with tensor-core usage) &
strong emphasis on mixed precision and throughput-oriented kernel organisation &
keep FP32 baseline, treat TF32 as opt-in path with fixed numerical validation; prioritise high-call-count kernels first \\
\cite{krinitsin2025ttn-ed} (TTN time evolution, Rizzi/Waintal collaboration) &
two-level parallelisation and GPU acceleration provide major practical gains in targeted regimes &
retain local-operator-sum style execution when possible and minimise explicit large intermediates that create extra memory traffic \\
\cite{wu2022chase-gpu,wu2023chase-nccl} (GPU-distributed dense eigensolver lineage at JSC) &
once local kernels are efficient, communication/orchestration limits dominate scaling &
for one-node scope, prepare kernels and APIs so later overlap of local compute and communication is possible without redesign \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Representative CUDA Implementation Patterns}\label{sec:sota-cuda-patterns}

The following snippets are representative implementation skeletons derived from
the above literature patterns. They are intentionally simplified to expose the
kernel-architecture idea.

\subsubsection{Pattern A: Bucketed Small-GEMM Execution (DMRG/MPS Sweeps)}

\begin{lstlisting}[language=CUDA, caption={Shape-bucketed small contractions using strided batched GEMM. Representative kernel-orchestration pattern inspired by massively parallel DMRG implementations.}, label={lst:sota-bucketed-small-gemm}]
struct BatchBucket {
    int m, n, k, count;
    const float* A;   // strided batch base
    const float* B;
    float* C;
    long long strideA, strideB, strideC;
};

void run_bucket(const BatchBucket& b, cublasHandle_t h, cudaStream_t s) {
    cublasSetStream(h, s);
    const float alpha = 1.0f, beta = 0.0f;
    cublasGemmStridedBatchedEx(
        h, CUBLAS_OP_N, CUBLAS_OP_N,
        b.m, b.n, b.k,
        &alpha,
        b.A, CUDA_R_32F, b.m, b.strideA,
        b.B, CUDA_R_32F, b.k, b.strideB,
        &beta,
        b.C, CUDA_R_32F, b.m, b.strideC,
        b.count,
        CUBLAS_COMPUTE_32F_FAST_TF32, CUBLAS_GEMM_DEFAULT_TENSOR_OP);
}
\end{lstlisting}

\textbf{Why it matters.}
Instead of launching one kernel per tiny contraction, this pattern executes
many contractions per call and shifts the regime away from launch-dominated
runtime.

\subsubsection{Pattern B: Operator-Sum Path Without Explicit MPO Materialisation}

\begin{lstlisting}[language=CUDA, caption={Operator-sum contraction sketch. The core idea is to accumulate local terms directly instead of materialising a large explicit operator tensor first.}, label={lst:sota-operator-sum}]
// Pseudocode-level sketch: each term is mapped to a GEMM-like contraction.
for (int t = 0; t < num_terms; ++t) {
    // left[t], right[t], and local_op[t] encode one local contribution
    contract_term_kernel<<<grid, block, shmem, stream>>>(
        left[t], local_op[t], right[t], state_in, state_tmp);
}

// Optional: fuse accumulation to reduce global-memory traffic.
accumulate_terms_kernel<<<grid2, block2, 0, stream>>>(state_tmp, state_out);
\end{lstlisting}

\textbf{Why it matters.}
It follows the local-operator-sum idea discussed in recent TTN benchmarking.
Intermediate tensor construction is reduced, and cache/memory-traffic behavior
often improves~\cite{krinitsin2025ttn-ed}.

\subsubsection{Pattern C: Overlap of Local Compute and Multi-GPU Reduction}

\begin{lstlisting}[language=CUDA, caption={Two-stream overlap pattern for distributed dense kernels. The communication stream runs asynchronous collectives while compute proceeds on the main stream.}, label={lst:sota-overlap}]
cudaStream_t s_compute, s_comm;
cudaStreamCreate(&s_compute);
cudaStreamCreate(&s_comm);

// 1) Local block update on each GPU
local_update_kernel<<<grid, block, 0, s_compute>>>(local_A, local_B, local_C);

// 2) Asynchronous inter-GPU reduction / exchange
ncclAllReduce(local_C, global_C, count, ncclFloat, ncclSum, comm, s_comm);

// 3) Synchronize only when data dependency requires it
cudaEvent_t done_comm;
cudaEventCreate(&done_comm);
cudaEventRecord(done_comm, s_comm);
cudaStreamWaitEvent(s_compute, done_comm, 0);
postprocess_kernel<<<grid2, block2, 0, s_compute>>>(global_C);
\end{lstlisting}

\textbf{Why it matters.}
It mirrors the communication/computation orchestration emphasis in
Wu--Di~Napoli style distributed GPU kernels and in massively parallel DMRG
implementations~\cite{wu2022chase-gpu,wu2023chase-nccl,menczer2023massive}.
