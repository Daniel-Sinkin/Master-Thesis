% chapters/02_sections/tensor_networks.tex
\section{Tensor Networks}\label{sec:tensor-networks}

\subsection{Historical Background}\label{sec:tn-history}

Tensor networks entered computational physics through the density-matrix
renormalization group (DMRG) in the early 1990s, where the key practical idea
was to keep only the most relevant subspace during iterative updates
~\cite{white1992dmrg,white1993dmrg}. During the 2000s, this was reformulated in
explicit tensor-network language, especially as matrix product states (MPS),
which clarified the relation between approximation quality and bond dimension
~\cite{schollwock2011dmrg,orus2014practical-intro-tn}.

For high-performance computing, the first generation of tensor-network codes
was CPU-focused and built around BLAS/LAPACK kernels plus MPI-based
distribution. Over time, the contraction-heavy parts moved to GPUs, where
cuBLAS/cuTENSOR-style kernels and communication-aware scheduling became central
for strong scaling across devices~\cite{springer2017tensor,menczer2023massive,menczer2024efficient-dmrg-gpu}.
In short: the algorithmic core stayed similar, but the performance bottlenecks
shifted toward memory movement, launch overhead, and multi-GPU orchestration.

\subsection{Why Tensor Networks Are of Interest}\label{sec:tn-interest}

Tensor networks are useful because they replace exponential state
representations with structured low-rank factorizations when entanglement is
limited. For a chain of $L$ sites with local dimension $d$, the full state
vector requires
\[
  N_{\text{state}} = d^L
\]
complex amplitudes. In contrast, a uniform-bond MPS uses
\[
  N_{\text{MPS}} \sim L\,d\,\chi^2,
\]
which is polynomial in system size for fixed bond dimension $\chi$.

This reduction is directly tied to entanglement structure. Across one bipartite
cut, an MPS corresponds to a truncated Schmidt decomposition
\[
  \left|\Psi\right\rangle = \sum_{a=1}^{\chi} s_a \left|a_L\right\rangle\left|a_R\right\rangle,
\]
and the von Neumann entropy
\[
  S = -\sum_a s_a^2 \log s_a^2
\]
is effectively controlled by the retained rank $\chi$.

From an HPC perspective, tensor networks are also interesting because they are a
stress test for modern GPUs: many dense contractions, repeated reshapes, and
long iterative workflows with short kernels. This combination makes them a good
case for studying both kernel-level efficiency and system-level orchestration
costs~\cite{zhou2020limits,ayral2023dmrg-circuit-fidelity}.

\subsection{Computational Context}\label{sec:tensor-context}

Tensor-network methods arise in quantum many-body simulation, where the full
state space of the Schr\"odinger equation is reduced to a structured set of
lower-rank objects. In this thesis, tensor networks are treated as an
\emph{application source of computational patterns}, not as a physics topic.
Detailed notation, model assumptions, and algorithmic physics choices are
outside scope here and are introduced separately with domain experts.

From a high-performance computing perspective, the relevant point is that
tensor-network codes repeatedly execute dense linear-algebra-like kernels on
moderate to small tensor blocks. Those kernels are often embedded in long
iterative workflows, so both per-kernel efficiency and orchestration overhead
matter.

\subsection{Matrix Product States (MPS) from a Computational View}
\label{sec:mps-computational-view}

Matrix Product States (MPS) are a common tensor-network representation where a
high-order object is factorised into a chain of local tensors. For this thesis,
the relevant point is not the physics interpretation but the resulting compute
structure:

\begin{itemize}
  \item repeated contraction of small and medium dense tensors,
  \item frequent reshape/permutation steps between contraction phases,
  \item recurring low-rank truncation steps (typically SVD/QR-based),
  \item many short kernel sequences within iterative sweeps.
\end{itemize}

Using a minimal notation, an order-$L$ MPS can be written as
\[
  \Psi(i_1,\dots,i_L)=
  \sum_{\alpha_1,\dots,\alpha_{L-1}}
  A^{[1]}_{i_1,\alpha_1}
  A^{[2]}_{\alpha_1,i_2,\alpha_2}
  \cdots
  A^{[L]}_{\alpha_{L-1},i_L},
\]
where $i_\ell$ is a physical index and $\alpha_\ell$ is a bond index. The bond
dimension $\chi$ controls both approximation quality and computational cost.

From an HPC perspective, two scaling facts are useful planning rules:
\begin{enumerate}
  \item storage for a uniform MPS scales as $\BigO(L\,d\,\chi^2)$ rather than
    exponentially in $L$,
  \item dominant kernels in common MPS updates scale roughly with
    $\BigO(d\,\chi^3)$ per local update step, so increasing $\chi$ quickly
    shifts pressure to memory traffic and tensor-factorisation kernels.
\end{enumerate}

A typical two-site update can be sketched with a simple cost model:
\[
  \text{FLOPs}_{\text{update}}
  \approx c_1\,d\,\chi^3 + c_2\,d^2\,\chi^2,
\]
where the first term represents contraction work and the second represents
truncation/factorization work. Constants $c_1,c_2$ depend on layout and update
scheme, but the scaling makes clear why $\chi$ is the dominant performance
driver.

In practical implementations, local updates are often executed as:
\begin{enumerate}
  \item fuse neighboring local tensors into an effective two-site tensor,
  \item contract with environment/operator terms,
  \item reshape to a matrix and run SVD/QR-style truncation,
  \item redistribute factors back into MPS form.
\end{enumerate}

This pattern maps directly to the optimisation targets in later chapters:
GEMM-like contractions, layout transforms, launch overhead control, and
profiling-guided tuning of memory/occupancy trade-offs.

\subsection{Kernel Patterns Relevant to This Thesis}\label{sec:tn-kernel-motifs}

The optimisation work in this thesis is driven by three recurring patterns:

\begin{enumerate}
  \item \textbf{Small and medium dense contractions.} After index reshaping,
    many contractions reduce to GEMM-like operations with dimensions that are
    too small to saturate the GPU when launched individually.
  \item \textbf{Layout transforms.} Permutations and reshapes are required to
    match library expectations (e.g.\ column-major interfaces), and these
    transforms can dominate runtime if memory access is not coalesced.
  \item \textbf{Fine-grained kernel sequences.} Practical workflows execute
    large numbers of short kernels; launch overhead and synchronisation
    overhead become first-order effects in this regime (see
    \cref{sec:kernel-launch-mechanics,sec:launch-overhead-benchmark}).
\end{enumerate}

This computational profile aligns directly with CUDA optimisation topics such as
occupancy control, shared-memory tiling, register pressure, launch fusion, and
host-device scheduling.

\subsection{Scope Boundary}\label{sec:tn-scope-boundary}

To keep the thesis focused and technically coherent, we explicitly exclude:

\begin{itemize}
  \item derivations of tensor-network physics models,
  \item detailed notation systems used in specific communities,
  \item algorithmic comparisons driven primarily by physics accuracy criteria.
\end{itemize}

Instead, we evaluate kernels by HPC criteria: runtime, achieved throughput,
memory efficiency, and scaling behaviour on Ampere-class GPUs.
