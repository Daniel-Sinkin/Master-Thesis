% chapters/02_sections/tensor_networks.tex
\section{Tensor Networks}\label{sec:tensor-networks}

\subsection{Historical Background}\label{sec:tn-history}

Tensor networks entered computational physics through the density-matrix
renormalization group (DMRG) in the early 1990s, where the key practical idea
was to keep only the most relevant subspace during iterative updates
~\cite{white1992dmrg,white1993dmrg}. During the 2000s, this was reformulated in
explicit tensor-network language, especially as matrix product states (MPS),
which clarified the relation between approximation quality and bond dimension
~\cite{schollwock2011dmrg,orus2014practical-intro-tn}.

For high-performance computing, the first generation of tensor-network codes
was CPU-focused and built around BLAS/LAPACK kernels plus MPI-based
distribution. Over time, the contraction-heavy parts moved to GPUs, where
cuBLAS/cuTENSOR-style kernels and communication-aware scheduling became central
for strong scaling across devices~\cite{springer2017tensor,menczer2023massive,menczer2024efficient-dmrg-gpu}.
The algorithmic core stayed similar, but the main performance bottlenecks
shifted toward memory movement and launch overhead, then to multi-GPU
orchestration at larger scale.

\subsection{Why Tensor Networks Are of Interest}\label{sec:tn-interest}

Tensor networks are useful because they replace exponential state
representations with structured low-rank factorizations when entanglement is
limited. For a chain of $L$ sites with local dimension $d$, the full state
vector requires
\[
  N_{\text{state}} = d^L
\]
complex amplitudes. In contrast, a uniform-bond MPS uses
\[
  N_{\text{MPS}} \sim L\,d\,\chi^2,
\]
which is polynomial in system size for fixed bond dimension $\chi$.

This reduction is directly tied to entanglement structure. Across one bipartite
cut, an MPS corresponds to a truncated Schmidt decomposition
\[
  \left|\Psi\right\rangle = \sum_{a=1}^{\chi} s_a \left|a_L\right\rangle\left|a_R\right\rangle,
\]
and the von Neumann entropy
\[
  S = -\sum_a s_a^2 \log s_a^2
\]
is effectively controlled by the retained rank $\chi$.

From an HPC perspective, tensor networks are also interesting because they
stress modern GPUs through dense contractions with repeated reshapes inside
iterative workflows dominated by short kernels. This makes them a good case for studying
kernel efficiency and orchestration costs together
~\cite{zhou2020limits,ayral2023dmrg-circuit-fidelity}.

\subsection{Computational Context}\label{sec:tensor-context}

Tensor-network methods arise in quantum many-body simulation, where the full
state space of the Schr\"odinger equation is reduced to a structured set of
lower-rank objects. In this thesis, tensor networks are treated as an
\emph{application source of computational patterns}, not as a physics topic.
Detailed notation and model-specific physics choices are outside scope here and
are introduced separately with domain experts.

From a high-performance computing perspective, the relevant point is that
tensor-network codes repeatedly execute dense linear-algebra-like kernels on
moderate to small tensor blocks. Those kernels are often embedded in long
iterative workflows, so both per-kernel efficiency and orchestration overhead
matter.

\subsection{Matrix Product States (MPS) from a Computational View}
\label{sec:mps-computational-view}

Matrix Product States (MPS) are a common tensor-network representation where a
high-order object is factorised into a chain of local tensors. For this thesis,
the relevant point is not the physics interpretation but the resulting compute
structure:

\begin{itemize}
  \item repeated contraction of small and medium dense tensors,
  \item frequent reshape/permutation steps between contraction phases,
  \item recurring low-rank truncation steps (typically SVD/QR-based),
  \item many short kernel sequences within iterative sweeps.
\end{itemize}

Using a minimal notation, an order-$L$ MPS can be written as
\[
  \Psi(i_1,\dots,i_L)=
  \sum_{\alpha_1,\dots,\alpha_{L-1}}
  A^{[1]}_{i_1,\alpha_1}
  A^{[2]}_{\alpha_1,i_2,\alpha_2}
  \cdots
  A^{[L]}_{\alpha_{L-1},i_L},
\]
where $i_\ell$ is a physical index and $\alpha_\ell$ is a bond index. The bond
dimension $\chi$ controls both approximation quality and computational cost.

From an HPC perspective, two scaling facts are useful planning rules:
\begin{enumerate}
  \item storage for a uniform MPS scales as $\BigO(L\,d\,\chi^2)$ rather than
    exponentially in $L$,
  \item dominant kernels in common MPS updates scale roughly with
    $\BigO(d\,\chi^3)$ per local update step, so increasing $\chi$ quickly
    shifts pressure to memory traffic and tensor-factorisation kernels.
\end{enumerate}

A typical two-site update can be sketched with a simple cost model:
\[
  \text{FLOPs}_{\text{update}}
  \approx c_1\,d\,\chi^3 + c_2\,d^2\,\chi^2,
\]
where the first term represents contraction work and the second represents
truncation/factorization work. Constants $c_1,c_2$ depend on layout and update
scheme, but the scaling makes clear why $\chi$ is the dominant performance
driver.

In practical implementations, local updates are often executed as:
\begin{enumerate}
  \item fuse neighboring local tensors into an effective two-site tensor,
  \item contract with environment/operator terms,
  \item reshape to a matrix and run SVD/QR-style truncation,
  \item redistribute factors back into MPS form.
\end{enumerate}

\begin{figure}[H]
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tikzpicture}[
    >=Latex,
    every node/.style={font=\small},
    phase/.style={draw, rounded corners=2pt, fill=diagBlue, minimum width=3.2cm, minimum height=0.95cm, align=center},
    kernel/.style={draw, rounded corners=2pt, fill=diagGreen, minimum width=3.2cm, minimum height=0.95cm, align=center},
    edge/.style={->, very thick},
    map/.style={->, thick, gray!70}
  ]
    \node[phase] (p1) at (0, 1.2) {Fuse two-site\\tensor};
    \node[phase] (p2) at (3.8, 1.2) {Contract with\\environment};
    \node[phase] (p3) at (7.6, 1.2) {Reshape and\\truncate};
    \node[phase] (p4) at (11.4, 1.2) {Redistribute\\factors};

    \node[kernel] (k1) at (0, 0) {Permutation /\\pack kernel};
    \node[kernel] (k2) at (3.8, 0) {GEMM-like\\contraction};
    \node[kernel] (k3) at (7.6, 0) {SVD / QR /\\orthogonalisation};
    \node[kernel] (k4) at (11.4, 0) {Writeback /\\layout restore};

    \draw[edge] (p1) -- (p2);
    \draw[edge] (p2) -- (p3);
    \draw[edge] (p3) -- (p4);

    \draw[edge] (k1) -- (k2);
    \draw[edge] (k2) -- (k3);
    \draw[edge] (k3) -- (k4);

    \draw[map] (p1) -- (k1);
    \draw[map] (p2) -- (k2);
    \draw[map] (p3) -- (k3);
    \draw[map] (p4) -- (k4);
  \end{tikzpicture}%
  }
  \caption{Two-site MPS update pipeline and its kernel-level mapping. This is
  a direct bridge between tensor-network algorithm steps (top) and GPU
  optimisation targets (bottom).}
  \label{fig:mps-two-site-kernel-mapping}
\end{figure}

The same structure appears in the optimisation targets of later chapters:
GEMM-like contractions and layout transforms, together with launch-overhead
control and profiling-guided tuning of memory/occupancy trade-offs.

\subsection{Kernel Patterns Relevant to This Thesis}\label{sec:tn-kernel-motifs}

The optimisation work in this thesis is driven by three recurring patterns:

\begin{enumerate}
  \item \textbf{Small and medium dense contractions.} After index reshaping,
    many contractions reduce to GEMM-like operations with dimensions that are
    too small to saturate the GPU when launched individually.
  \item \textbf{Layout transforms.} Permutations and reshapes are required to
    match library expectations (e.g.\ column-major interfaces), and these
    transforms can dominate runtime if memory access is not coalesced.
  \item \textbf{Fine-grained kernel sequences.} Practical workflows execute
    large numbers of short kernels; launch overhead and synchronisation
    overhead become first-order effects in this regime (see
    \cref{sec:kernel-launch-mechanics,sec:launch-overhead-benchmark}).
\end{enumerate}

The profile aligns directly with CUDA topics such as occupancy control,
shared-memory tiling and launch fusion, plus host-device scheduling.

\subsection{Scope Boundary}\label{sec:tn-scope-boundary}

To keep the thesis focused and technically coherent, we explicitly exclude:

\begin{itemize}
  \item derivations of tensor-network physics models,
  \item detailed notation systems used in specific communities,
  \item algorithmic comparisons driven primarily by physics accuracy criteria.
\end{itemize}

Instead, kernels are evaluated by HPC criteria: runtime, achieved throughput,
memory efficiency, and scaling behaviour on Ampere-class GPUs.
