% chapters/02_sections/tensor_networks.tex
\section{Tensor Networks}\label{sec:tensor-networks}

\subsection{Computational Context}\label{sec:tensor-context}

Tensor-network methods arise in quantum many-body simulation, where the full
state space of the Schr\"odinger equation is reduced to a structured set of
lower-rank objects. In this thesis, tensor networks are treated as an
\emph{application source of computational patterns}, not as a physics topic.
Detailed notation, model assumptions, and algorithmic physics choices are
outside scope here and are introduced separately with domain experts.

From a high-performance computing perspective, the relevant point is that
tensor-network codes repeatedly execute dense linear-algebra-like kernels on
moderate to small tensor blocks. Those kernels are often embedded in long
iterative workflows, so both per-kernel efficiency and orchestration overhead
matter.

\subsection{Matrix Product States (MPS) from a Computational View}
\label{sec:mps-computational-view}

Matrix Product States (MPS) are a common tensor-network representation where a
high-order object is factorised into a chain of local tensors. For this thesis,
the relevant point is not the physics interpretation but the resulting compute
structure:

\begin{itemize}
  \item repeated contraction of small and medium dense tensors,
  \item frequent reshape/permutation steps between contraction phases,
  \item recurring low-rank truncation steps (typically SVD/QR-based),
  \item many short kernel sequences within iterative sweeps.
\end{itemize}

Using a minimal notation, an order-$L$ MPS can be written as
\[
  \Psi(i_1,\dots,i_L)=
  \sum_{\alpha_1,\dots,\alpha_{L-1}}
  A^{[1]}_{i_1,\alpha_1}
  A^{[2]}_{\alpha_1,i_2,\alpha_2}
  \cdots
  A^{[L]}_{\alpha_{L-1},i_L},
\]
where $i_\ell$ is a physical index and $\alpha_\ell$ is a bond index. The bond
dimension $\chi$ controls both approximation quality and computational cost.

From an HPC perspective, two scaling facts are useful planning rules:
\begin{enumerate}
  \item storage for a uniform MPS scales as $\BigO(L\,d\,\chi^2)$ rather than
    exponentially in $L$,
  \item dominant kernels in common MPS updates scale roughly with
    $\BigO(d\,\chi^3)$ per local update step, so increasing $\chi$ quickly
    shifts pressure to memory traffic and tensor-factorisation kernels.
\end{enumerate}

In practical implementations, local updates are often executed as:
\begin{enumerate}
  \item fuse neighboring local tensors into an effective two-site tensor,
  \item contract with environment/operator terms,
  \item reshape to a matrix and run SVD/QR-style truncation,
  \item redistribute factors back into MPS form.
\end{enumerate}

This pattern maps directly to the optimisation targets in later chapters:
GEMM-like contractions, layout transforms, launch overhead control, and
profiling-guided tuning of memory/occupancy trade-offs.

\subsection{Kernel Motifs Relevant to This Thesis}\label{sec:tn-kernel-motifs}

The optimisation work in this thesis is driven by three recurring motifs:

\begin{enumerate}
  \item \textbf{Small and medium dense contractions.} After index reshaping,
    many contractions reduce to GEMM-like operations with dimensions that are
    too small to saturate the GPU when launched individually.
  \item \textbf{Layout transforms.} Permutations and reshapes are required to
    match library expectations (e.g.\ column-major interfaces), and these
    transforms can dominate runtime if memory access is not coalesced.
  \item \textbf{Fine-grained kernel sequences.} Practical workflows execute
    large numbers of short kernels; launch overhead and synchronisation
    overhead become first-order effects in this regime (see
    \cref{sec:kernel-launch-mechanics,sec:launch-overhead-benchmark}).
\end{enumerate}

This computational profile aligns directly with CUDA optimisation topics such as
occupancy control, shared-memory tiling, register pressure, launch fusion, and
host-device scheduling.

\subsection{Scope Boundary}\label{sec:tn-scope-boundary}

To keep the thesis focused and technically coherent, we explicitly exclude:

\begin{itemize}
  \item derivations of tensor-network physics models,
  \item detailed notation systems used in specific communities,
  \item algorithmic comparisons driven primarily by physics accuracy criteria.
\end{itemize}

Instead, we evaluate kernels by HPC criteria: runtime, achieved throughput,
memory efficiency, and scaling behaviour on Ampere-class GPUs.
